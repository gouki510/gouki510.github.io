<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ReadHub | Gouki Minegishi </title> <meta name="author" content="Gouki Minegishi"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%A3&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gouki510.github.io/books/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gouki</span> Minegishi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item active" href="/books/">ReadHub</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <blockquote> <p>What an astonishing thing a book is. It’s a flat object made from a tree with flexible parts on which are imprinted lots of funny dark squiggles. But one glance at it and you’re inside the mind of another person, maybe somebody dead for thousands of years. Across the millennia, an author is speaking clearly and silently inside your head, directly to you. Writing is perhaps the greatest of human inventions, binding together people who never knew each other, citizens of distant epochs. Books break the shackles of time. A book is proof that humans are capable of working magic.</p> <p>– Carl Sagan, Cosmos, Part 11: The Persistence of Memory (1980)</p> </blockquote> <h2 id="papers-that-i-am-reading-have-read-or-will-read">Papers that I am reading, have read, or will read</h2> <ul> <li> <p><strong><a href="https://arxiv.org/abs/2505.19590" rel="external nofollow noopener" target="_blank">Learning to Reason without External Rewards</a></strong><br> 一様分布とモデルの出力分布の最大化(≒モデルのconfidenceの最大化)でRLしても結構学習できちゃう Aha momentは起きてるかはわからない（明示的に書いていないから起きてなさそう）</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2505.15105" rel="external nofollow noopener" target="_blank">Mechanistic evaluation of Transformers and state space models</a></strong><br> SSM系がTransformerより性能が低いのは，induction headが学習できないから（と言っているように見える）． Linear AttentionとSliding Windowを組み合わせたBASEDは学習できる． ただタスクがAR系のみなのでそこはLimitationか．</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2502.05475" rel="external nofollow noopener" target="_blank">AI Alignment Requires Understanding How Data Shapes Structure and Generalisation</a></strong><br> データの統計的性質は，loss landscapeの形状や学習プロセス，モデルが獲得したアルゴリズムに間接的に影響を与えるが，間接的なので同じデータを与えてもモデルの振る舞いが異なる時がある．（つまりデータの性質だけではモデルの振る舞いは予測できない）なので，本質的なアライメントのためには，モデル内部を知る必要があるという主張．（道具はSLT）</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2504.14379" rel="external nofollow noopener" target="_blank">The Geometry of Self-Verification in a Task-Specific Reasoning Model</a></strong><br> Probingを使って，self-verificationに寄与しているGLUの重み(2層MLPの2層目に相当)を見つける．層の後半に多い．self-verificationする/しないのベクトルは逆を向いている．Intervention実験から，また，Attentionにおいてはprevious-token headが重要ということがわかった．層の前半に多い，(previous-token headはinduction headの1層目のhead)．つまり，いくつかのhead(論文では3つ)がself-verificationを起こすGLUのvectorをactivateさせている（GLUは入力依存なのでこれが可能）．Base Modelにもこの分析は通用する，つまりbase modelにすでに備わっているself-verificationの能力をRL-FTで強化しているのでは？ということ</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2412.17034" rel="external nofollow noopener" target="_blank">Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models</a></strong><br> Jailbreakの仕組み(ACL’25)．モデル内部にHarmfulな内容をrepresentした活性化空間がありクラスターになっている，Jailbreakはこのクラスタの境界(safety boundary)を内から外に跨ぐような攻撃．（Harmfulな活性値を良好な活性値の空間にしてしまう）この分析から新たな防御手法を提案</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2505.18373" rel="external nofollow noopener" target="_blank">Next-token pretraining implies in-context learning</a></strong><br> Next Token PredictionでICLの能力が獲得される理論．ウェブデータのような非エルゴート的なデータの場合，次のトークンのCEを下げようとすると，モデルはコンテキストからどのデータソース（エルゴード成分）からのシーケンスであるかを識別する必要があり，この「成分の曖昧さの解消」プロセスがICLのメカニズムに重要．</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2505.13763" rel="external nofollow noopener" target="_blank">Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations</a></strong><br> LLMは自分の活性値をモニタリング/制御できる．ニューロフィードバックを用いた実験．</p> </li> <li> <p><strong><a href="https://livgorton.com/adversarial-examples-superposition/" rel="external nofollow noopener" target="_blank">Adversarial Examples Aren’t Bugs, They’re… Superposition?</a></strong><br> 敵対的攻撃のmajorな原因はsuperpositionだという主張．敵対的攻撃に頑健なモデルはSAEのlossが低い（≒敵対的攻撃に強いと表現の重ね合わせが少ない，敵対的攻撃に弱いと表現の重ね合わせが激しい）</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2502.02013" rel="external nofollow noopener" target="_blank">Layer by Layer: Uncovering Hidden Representations in Language Models</a></strong><br> ICML’25 spotlight，著者にルカンがいる．token embedding(N x D)のグラム行列(N x N)の固有値のエントロピーをメトリクスとして，内部表現の質を図る．（この指標が低いと少数の固有値が支配的で表現が圧縮されているということ）decoder-onlyのLLMでは，中間層でガクッと下がる(表現が良くなる)．BERT,mambaはそうならない．これはダウンストリーミングタスクの性能とも対応する．</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2503.13431" rel="external nofollow noopener" target="_blank">Measuring In-Context Computation Complexity via Hidden State Prediction</a></strong><br> LLMの内部状態を予測するモデルを後から取り付けることで，内部の計算複雑性を測る．（著者にシュミット・フーバーがいる）</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2408.12578" rel="external nofollow noopener" target="_blank">A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language</a></strong><br> 創発現象は概念の浸透によって起きる．（Hidenori Tanaka）</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2406.16254" rel="external nofollow noopener" target="_blank">Confidence Regulation Neurons in Language Models</a></strong><br> LLMは最後のLayer Normの直前でunembedding行列の全ての列ベクトルに直交する方向のベクトルを使って確信度を調整している．その方向を強めることがtemplatureを操作することに近い</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2406.11717" rel="external nofollow noopener" target="_blank">Refusal in Language Models Is Mediated by a Single Direction</a></strong><br> (Harmfulな入力の活性値)-(harmlessな入力の活性値)でrefusal directionが得られる．それを使って介入すると簡単にjailbreakができる</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2502.11019" rel="external nofollow noopener" target="_blank">Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning</a></strong><br> LLMのCatastrophic forgetting（破滅的忘却）は，特定の知識/能力が上書きされているわけでなく，内部に存在しているがそれを「呼び出す方法」を忘れている．Activation patching (Function Vector)を用いて分析．評価がめちゃ高い</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2410.137879" rel="external nofollow noopener" target="_blank">Looking Inward: Language Models Can Learn About Themselves by Introspection</a></strong><br> LLMは内省能力があるか？あるらしい．他のモデルより自分の内部状態について自分で当てられる</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2404.17563" rel="external nofollow noopener" target="_blank">An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem</a></strong><br> 段階的なスキルの獲得によって滑らかなscaling lawが説明できる</p> </li> </ul> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gouki Minegishi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>